{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data exploration, preprocessing and feature engineering</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this and the following notebooks we will demonstrate how you can build your ML Pipeline leveraging SKLearn Feature Transformers and SageMaker XGBoost algorithm & after the model is trained, deploy the Pipeline (Feature Transformer and XGBoost) as a SageMaker Inference Pipeline behind a single Endpoint for real-time inference.\n",
    "\n",
    "In particular, in this notebook we will tackle the first steps related to data exploration and preparation. We will use [Amazon Athena](https://aws.amazon.com/athena/) to query our dataset and have a first insight about data quality and available features, [AWS Glue](https://aws.amazon.com/glue/) to create a Data Catalog and [Amazon SageMaker Processing](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html) for building the feature transformer model with SKLearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/end-to-end-ml-sm/02_data_exploration_and_feature_eng\n"
     ]
    }
   ],
   "source": [
    "# When using Amazon SageMaker Studio, please set this variable to True and execute the cell\n",
    "use_sm_studio = True\n",
    "if use_sm_studio:\n",
    "    %cd /root/end-to-end-ml-sm/02_data_exploration_and_feature_eng/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.23.5\n"
     ]
    }
   ],
   "source": [
    "# Check SageMaker Python SDK version\n",
    "import sagemaker\n",
    "print(sagemaker.__version__)\n",
    "\n",
    "def versiontuple(v):\n",
    "    return tuple(map(int, (v.split(\".\"))))\n",
    "\n",
    "if versiontuple(sagemaker.__version__) < versiontuple('2.5.0'):\n",
    "    raise Exception(\"This notebook requires at least SageMaker Python SDK version 2.5.0. Please install it via pip.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-east-1\n",
      "arn:aws:iam::041631420165:role/service-role/AmazonSageMaker-ExecutionRole-endtoendml\n",
      "sagemaker-us-east-1-041631420165\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import time\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "prefix = 'endtoendmlsm'\n",
    "\n",
    "print(region)\n",
    "print(role)\n",
    "print(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now copy to our bucket the dataset used for this use case. We will use the `windturbine_raw_data_header.csv` made available for this workshop in the `gianpo-public` public S3 bucket. In this Notebook, we will download from that bucket and upload to your bucket so that AWS services can access the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "file_key = 'data/raw/windturbine_raw_data_header.csv'\n",
    "copy_source = {\n",
    "    'Bucket': 'gianpo-public',\n",
    "    'Key': 'endtoendml/{0}'.format(file_key)\n",
    "}\n",
    "\n",
    "s3.Bucket(bucket_name).copy(copy_source, '{0}/'.format(prefix) + file_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need now is to infer a schema for our dataset. Thanks to its [integration with AWS Glue](https://docs.aws.amazon.com/athena/latest/ug/glue-athena.html), we will later use Amazon Athena to run SQL queries against our data stored in S3 without the need to import them into a relational database. To do so, Amazon Athena uses the AWS Glue Data Catalog as a central location to store and retrieve table metadata throughout an AWS account. The Athena execution engine, indeed, requires table metadata that instructs it where to read data, how to read it, and other information necessary to process the data.\n",
    "\n",
    "To organize our Glue Data Catalog we create a new database named `endtoendml-db`. To do so, we create a Glue client via Boto and invoke the `create_database` method.\n",
    "\n",
    "However, first we want to make sure these AWS resources to not exist yet to avoid any error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanup completed.\n"
     ]
    }
   ],
   "source": [
    "from notebook_utilities import cleanup_glue_resources\n",
    "cleanup_glue_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client = boto3.client('glue')\n",
    "\n",
    "response = glue_client.create_database(DatabaseInput={'Name': 'endtoendml-db'})\n",
    "response = glue_client.get_database(Name='endtoendml-db')\n",
    "response\n",
    "assert response['Database']['Name'] == 'endtoendml-db'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a Glue Crawler that we point to the S3 path where the dataset resides, and the crawler creates table definitions in the Data Catalog.\n",
    "To grant the correct set of access permission to the crawler, we use one of the roles created before (`GlueServiceRole-endtoendml`) whose policy grants AWS Glue access to data stored in your S3 buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = glue_client.create_crawler(\n",
    "    Name='endtoendml-crawler',\n",
    "    Role='service-role/GlueServiceRole-endtoendml', \n",
    "    DatabaseName='endtoendml-db',\n",
    "    Targets={'S3Targets': [{'Path': '{0}/{1}/data/raw/'.format(bucket_name, prefix)}]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to run the crawler with the `start_crawler` API and to monitor its status upon completion through the `get_crawler_metrics` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n"
     ]
    }
   ],
   "source": [
    "glue_client.start_crawler(Name='endtoendml-crawler')\n",
    "\n",
    "while glue_client.get_crawler_metrics(CrawlerNameList=['endtoendml-crawler'])['CrawlerMetricsList'][0]['TablesCreated'] == 0:\n",
    "    print('RUNNING')\n",
    "    time.sleep(15)\n",
    "    \n",
    "assert glue_client.get_crawler_metrics(CrawlerNameList=['endtoendml-crawler'])['CrawlerMetricsList'][0]['TablesCreated'] == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the crawler has finished its job, we can retrieve the Table definition for the newly created table.\n",
    "As you can see, the crawler has been able to correctly identify 12 fields, infer a type for each column and assign a name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Table': {'Name': 'raw',\n",
       "  'DatabaseName': 'endtoendml-db',\n",
       "  'Owner': 'owner',\n",
       "  'CreateTime': datetime.datetime(2021, 2, 3, 7, 5, 36, tzinfo=tzlocal()),\n",
       "  'UpdateTime': datetime.datetime(2021, 2, 3, 7, 5, 36, tzinfo=tzlocal()),\n",
       "  'LastAccessTime': datetime.datetime(2021, 2, 3, 7, 5, 36, tzinfo=tzlocal()),\n",
       "  'Retention': 0,\n",
       "  'StorageDescriptor': {'Columns': [{'Name': 'turbine_id', 'Type': 'string'},\n",
       "    {'Name': 'turbine_type', 'Type': 'string'},\n",
       "    {'Name': 'wind_speed', 'Type': 'bigint'},\n",
       "    {'Name': 'rpm_blade', 'Type': 'bigint'},\n",
       "    {'Name': 'oil_temperature', 'Type': 'double'},\n",
       "    {'Name': 'oil_level', 'Type': 'bigint'},\n",
       "    {'Name': 'temperature', 'Type': 'bigint'},\n",
       "    {'Name': 'humidity', 'Type': 'bigint'},\n",
       "    {'Name': 'vibrations_frequency', 'Type': 'bigint'},\n",
       "    {'Name': 'pressure', 'Type': 'bigint'},\n",
       "    {'Name': 'wind_direction', 'Type': 'string'},\n",
       "    {'Name': 'breakdown', 'Type': 'string'}],\n",
       "   'Location': 's3://sagemaker-us-east-1-041631420165/endtoendmlsm/data/raw/',\n",
       "   'InputFormat': 'org.apache.hadoop.mapred.TextInputFormat',\n",
       "   'OutputFormat': 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',\n",
       "   'Compressed': False,\n",
       "   'NumberOfBuckets': -1,\n",
       "   'SerdeInfo': {'SerializationLibrary': 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe',\n",
       "    'Parameters': {'field.delim': ','}},\n",
       "   'BucketColumns': [],\n",
       "   'SortColumns': [],\n",
       "   'Parameters': {'CrawlerSchemaDeserializerVersion': '1.0',\n",
       "    'CrawlerSchemaSerializerVersion': '1.0',\n",
       "    'UPDATED_BY_CRAWLER': 'endtoendml-crawler',\n",
       "    'areColumnsQuoted': 'false',\n",
       "    'averageRecordSize': '75',\n",
       "    'classification': 'csv',\n",
       "    'columnsOrdered': 'true',\n",
       "    'compressionType': 'none',\n",
       "    'delimiter': ',',\n",
       "    'objectCount': '1',\n",
       "    'recordCount': '564265',\n",
       "    'sizeKey': '42319941',\n",
       "    'skip.header.line.count': '1',\n",
       "    'typeOfData': 'file'},\n",
       "   'StoredAsSubDirectories': False},\n",
       "  'PartitionKeys': [],\n",
       "  'TableType': 'EXTERNAL_TABLE',\n",
       "  'Parameters': {'CrawlerSchemaDeserializerVersion': '1.0',\n",
       "   'CrawlerSchemaSerializerVersion': '1.0',\n",
       "   'UPDATED_BY_CRAWLER': 'endtoendml-crawler',\n",
       "   'areColumnsQuoted': 'false',\n",
       "   'averageRecordSize': '75',\n",
       "   'classification': 'csv',\n",
       "   'columnsOrdered': 'true',\n",
       "   'compressionType': 'none',\n",
       "   'delimiter': ',',\n",
       "   'objectCount': '1',\n",
       "   'recordCount': '564265',\n",
       "   'sizeKey': '42319941',\n",
       "   'skip.header.line.count': '1',\n",
       "   'typeOfData': 'file'},\n",
       "  'CreatedBy': 'arn:aws:sts::041631420165:assumed-role/GlueServiceRole-endtoendml/AWS-Crawler',\n",
       "  'IsRegisteredWithLakeFormation': False,\n",
       "  'CatalogId': '041631420165'},\n",
       " 'ResponseMetadata': {'RequestId': '85668fc0-0df6-4cd9-9f13-7a404bd58d9d',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Wed, 03 Feb 2021 07:05:59 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '2123',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': '85668fc0-0df6-4cd9-9f13-7a404bd58d9d'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = glue_client.get_table(DatabaseName='endtoendml-db', Name='raw')\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our knowledge of the dataset, we can be more specific with column names and types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '52a54cae-43e9-4226-aadf-90b3159b9428',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Wed, 03 Feb 2021 07:08:28 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '2',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': '52a54cae-43e9-4226-aadf-90b3159b9428'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have to remove the CatalogId key from the dictionary due to an breaking change\n",
    "# intrduced in botocore 1.17.18.\n",
    "del table['Table']['CatalogId']\n",
    "\n",
    "table['Table']['StorageDescriptor']['Columns'] = [{'Name': 'turbine_id', 'Type': 'string'},\n",
    "                                                  {'Name': 'turbine_type', 'Type': 'string'},\n",
    "                                                  {'Name': 'wind_speed', 'Type': 'double'},\n",
    "                                                  {'Name': 'rpm_blade', 'Type': 'double'},\n",
    "                                                  {'Name': 'oil_temperature', 'Type': 'double'},\n",
    "                                                  {'Name': 'oil_level', 'Type': 'double'},\n",
    "                                                  {'Name': 'temperature', 'Type': 'double'},\n",
    "                                                  {'Name': 'humidity', 'Type': 'double'},\n",
    "                                                  {'Name': 'vibrations_frequency', 'Type': 'double'},\n",
    "                                                  {'Name': 'pressure', 'Type': 'double'},\n",
    "                                                  {'Name': 'wind_direction', 'Type': 'string'},\n",
    "                                                  {'Name': 'breakdown', 'Type': 'string'}]\n",
    "updated_table = table['Table']\n",
    "updated_table.pop('DatabaseName', None)\n",
    "updated_table.pop('CreateTime', None)\n",
    "updated_table.pop('UpdateTime', None)\n",
    "updated_table.pop('CreatedBy', None)\n",
    "updated_table.pop('IsRegisteredWithLakeFormation', None)\n",
    "\n",
    "glue_client.update_table(\n",
    "    DatabaseName='endtoendml-db',\n",
    "    TableInput=updated_table\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data exploration with Amazon Athena</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data exploration, let's install PyAthena, a Python client for Amazon Athena. Note: PyAthena is not maintained by AWS, please visit: https://pypi.org/project/PyAthena/ for additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.7/site-packages (0.5.2)\n",
      "Requirement already satisfied: aiobotocore>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from s3fs) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from s3fs) (0.8.5)\n",
      "Collecting botocore<1.19.53,>=1.19.52\n",
      "  Downloading botocore-1.19.52-py2.py3-none-any.whl (7.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.2 MB 23.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: aiohttp>=3.3.1 in /opt/conda/lib/python3.7/site-packages (from aiobotocore>=1.0.1->s3fs) (3.7.3)\n",
      "Requirement already satisfied: wrapt>=1.10.10 in /opt/conda/lib/python3.7/site-packages (from aiobotocore>=1.0.1->s3fs) (1.11.2)\n",
      "Requirement already satisfied: aioitertools>=0.5.1 in /opt/conda/lib/python3.7/site-packages (from aiobotocore>=1.0.1->s3fs) (0.7.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.19.53,>=1.19.52->aiobotocore>=1.0.1->s3fs) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.19.53,>=1.19.52->aiobotocore>=1.0.1->s3fs) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4; python_version != \"3.4\" in /opt/conda/lib/python3.7/site-packages (from botocore<1.19.53,>=1.19.52->aiobotocore>=1.0.1->s3fs) (1.25.8)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (1.6.3)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (3.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (19.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (5.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (3.7.4.3)\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (3.0.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.19.53,>=1.19.52->aiobotocore>=1.0.1->s3fs) (1.14.0)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (2.8)\n",
      "\u001b[31mERROR: boto3 1.16.56 has requirement botocore<1.20.0,>=1.19.56, but you'll have botocore 1.19.52 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: awscli 1.18.216 has requirement botocore==1.19.56, but you'll have botocore 1.19.52 which is incompatible.\u001b[0m\n",
      "Installing collected packages: botocore\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.19.56\n",
      "    Uninstalling botocore-1.19.56:\n",
      "      Successfully uninstalled botocore-1.19.56\n",
      "Successfully installed botocore-1.19.52\n",
      "Collecting pyathena\n",
      "  Downloading PyAthena-2.1.1-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: boto3>=1.4.4 in /opt/conda/lib/python3.7/site-packages (from pyathena) (1.16.56)\n",
      "Requirement already satisfied: botocore>=1.5.52 in /opt/conda/lib/python3.7/site-packages (from pyathena) (1.19.52)\n",
      "Collecting tenacity>=4.1.0\n",
      "  Downloading tenacity-6.3.1-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.4.4->pyathena) (0.3.4)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.4.4->pyathena) (0.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4; python_version != \"3.4\" in /opt/conda/lib/python3.7/site-packages (from botocore>=1.5.52->pyathena) (1.25.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore>=1.5.52->pyathena) (2.8.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from tenacity>=4.1.0->pyathena) (1.14.0)\n",
      "Installing collected packages: tenacity, pyathena\n",
      "Successfully installed pyathena-2.1.1 tenacity-6.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install s3fs\n",
    "!pip install pyathena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>turbine_id</th>\n",
       "      <th>turbine_type</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>rpm_blade</th>\n",
       "      <th>oil_temperature</th>\n",
       "      <th>oil_level</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "      <th>vibrations_frequency</th>\n",
       "      <th>pressure</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>breakdown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TID003</td>\n",
       "      <td>HAWT</td>\n",
       "      <td>80.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>E</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TID010</td>\n",
       "      <td>HAWT</td>\n",
       "      <td>85.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>NE</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TID007</td>\n",
       "      <td>HAWT</td>\n",
       "      <td>47.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>N</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TID008</td>\n",
       "      <td>VAWT</td>\n",
       "      <td>73.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>SW</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TID003</td>\n",
       "      <td>HAWT</td>\n",
       "      <td>16.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>W</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TID001</td>\n",
       "      <td>HAWT</td>\n",
       "      <td>78.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>SW</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TID009</td>\n",
       "      <td>HAWT</td>\n",
       "      <td>80.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TID002</td>\n",
       "      <td>VAWT</td>\n",
       "      <td>59.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>SE</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  turbine_id turbine_type  wind_speed  rpm_blade  oil_temperature  oil_level  \\\n",
       "0     TID003         HAWT        80.0       61.0              NaN       34.0   \n",
       "1     TID010         HAWT        85.0       78.0             36.0       28.0   \n",
       "2     TID007         HAWT        47.0       31.0             31.0       23.0   \n",
       "3     TID008         VAWT        73.0       70.0             38.0        8.0   \n",
       "4     TID003         HAWT        16.0       23.0             46.0        9.0   \n",
       "5     TID001         HAWT        78.0       71.0             30.0       11.0   \n",
       "6     TID009         HAWT        80.0       25.0             37.0       31.0   \n",
       "7     TID002         VAWT        59.0       29.0             37.0       10.0   \n",
       "\n",
       "   temperature  humidity  vibrations_frequency  pressure wind_direction  \\\n",
       "0         33.0      26.0                   1.0      77.0              E   \n",
       "1         35.0      43.0                  15.0      62.0             NE   \n",
       "2         46.0      62.0                  15.0      32.0              N   \n",
       "3         17.0      66.0                   6.0      80.0             SW   \n",
       "4         76.0      53.0                  14.0      29.0              W   \n",
       "5         66.0      79.0                   1.0      81.0             SW   \n",
       "6         40.0      75.0                   4.0      56.0             NW   \n",
       "7         25.0      83.0                  13.0      55.0             SE   \n",
       "\n",
       "  breakdown  \n",
       "0        no  \n",
       "1       yes  \n",
       "2        no  \n",
       "3       yes  \n",
       "4        no  \n",
       "5        no  \n",
       "6        no  \n",
       "7        no  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyathena\n",
    "from pyathena import connect\n",
    "import pandas as pd\n",
    "\n",
    "athena_cursor = connect(s3_staging_dir='s3://{0}/{1}/staging/'.format(bucket_name, prefix), \n",
    "                        region_name=region).cursor()\n",
    "\n",
    "athena_cursor.execute('SELECT * FROM \"endtoendml-db\".raw limit 8;')\n",
    "pd.read_csv(athena_cursor.output_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another SQL query to count how many records we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_col0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     _col0\n",
       "0  1000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "athena_cursor.execute('SELECT COUNT(*) FROM \"endtoendml-db\".raw;')\n",
    "pd.read_csv(athena_cursor.output_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to see what are possible values for the field \"breakdown\" and how frequently they occur over the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>breakdown</th>\n",
       "      <th>percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no</td>\n",
       "      <td>86.3421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yes</td>\n",
       "      <td>13.6579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  breakdown  percent\n",
       "0        no  86.3421\n",
       "1       yes  13.6579"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "athena_cursor.execute('SELECT breakdown, (COUNT(breakdown) * 100.0 / (SELECT COUNT(*) FROM \"endtoendml-db\".raw)) \\\n",
    "            AS percent FROM \"endtoendml-db\".raw GROUP BY breakdown;')\n",
    "pd.read_csv(athena_cursor.output_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 2 artists>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "athena_cursor.execute('SELECT breakdown, COUNT(breakdown) AS bd_count FROM \"endtoendml-db\".raw GROUP BY breakdown;')\n",
    "df = pd.read_csv(athena_cursor.output_location)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(df.breakdown, df.bd_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have discovered that the dataset is quite unbalanced, although we are not going to try balancing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>turbine_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HAWT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VAWT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  turbine_type\n",
       "0         HAWT\n",
       "1         VAWT\n",
       "2          NaN"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "athena_cursor.execute('SELECT DISTINCT(turbine_type) FROM \"endtoendml-db\".raw')\n",
    "pd.read_csv(athena_cursor.output_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_col0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _col0\n",
       "0  38297"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "athena_cursor.execute('SELECT COUNT(*) FROM \"endtoendml-db\".raw WHERE oil_temperature IS NULL GROUP BY oil_temperature')\n",
    "pd.read_csv(athena_cursor.output_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also realized there are a few null values that need to be managed during the data preparation steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of keeping the data exploration step short during the workshop, we are not going to execute additional queries. However, feel free to explore the dataset more if you have time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: you can go to Amazon Athena console and check for query duration under History tab: usually queries are executed in a few seconds, then it some time for Pandas to load results into a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting started with preprocessing and feature engineering, we want to leverage on Amazon SageMaker Experiments to track the experimentations that we will be executing.\n",
    "We are going to create a new experiment and then a new trial, that represents a multi-step ML workflow (e.g. preprocessing stage1, preprocessing stage2, training stage, etc.). Each step of a trial maps to a trial component in SageMaker Experiments.\n",
    "\n",
    "We will use the Amazon SageMaker Experiments SDK to interact with the service from the notebooks. Additional info and documentation is available here: https://github.com/aws/sagemaker-experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker-experiments\n",
      "  Downloading sagemaker_experiments-0.1.26-py3-none-any.whl (41 kB)\n",
      "\u001b[K     |████████████████████████████████| 41 kB 29 kB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: boto3>=1.16.27 in /opt/conda/lib/python3.7/site-packages (from sagemaker-experiments) (1.16.56)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.16.27->sagemaker-experiments) (0.10.0)\n",
      "Collecting botocore<1.20.0,>=1.19.56\n",
      "  Downloading botocore-1.19.63-py2.py3-none-any.whl (7.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.2 MB 24.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.16.27->sagemaker-experiments) (0.3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4; python_version != \"3.4\" in /opt/conda/lib/python3.7/site-packages (from botocore<1.20.0,>=1.19.56->boto3>=1.16.27->sagemaker-experiments) (1.25.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.20.0,>=1.19.56->boto3>=1.16.27->sagemaker-experiments) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.56->boto3>=1.16.27->sagemaker-experiments) (1.14.0)\n",
      "\u001b[31mERROR: awscli 1.18.216 has requirement botocore==1.19.56, but you'll have botocore 1.19.63 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: aiobotocore 1.2.0 has requirement botocore<1.19.53,>=1.19.52, but you'll have botocore 1.19.63 which is incompatible.\u001b[0m\n",
      "Installing collected packages: sagemaker-experiments, botocore\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.19.52\n",
      "    Uninstalling botocore-1.19.52:\n",
      "      Successfully uninstalled botocore-1.19.52\n",
      "Successfully installed botocore-1.19.63 sagemaker-experiments-0.1.26\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker-experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are creating the experiment, or loading if it already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end-to-end-ml-sagemaker-1612336463\n",
      "Experiment created.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from smexperiments import experiment\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "experiment_name = 'end-to-end-ml-sagemaker-{0}'.format(str(int(time.time())))\n",
    "print(experiment_name)\n",
    "current_experiment = None\n",
    "\n",
    "try:\n",
    "    current_experiment = experiment.Experiment.load(experiment_name)\n",
    "    print('Experiment loaded.')\n",
    "except ClientError as ex:\n",
    "    if ex.response['Error']['Code'] == 'ResourceNotFound':\n",
    "        # Create experiment\n",
    "        current_experiment = experiment.Experiment.create(experiment_name=experiment_name,\n",
    "                                                          description='SageMaker workshop experiment')\n",
    "        print('Experiment created.')\n",
    "    else:\n",
    "        raise ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our experiment, we can create a new trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "trial_name = 'sklearn-xgboost-{0}'.format(str(int(time.time())))\n",
    "current_trial = current_experiment.create_trial(trial_name=trial_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now own, we will use the experiment and the trial as configuration parameters for the preprocessing and training jobs, to make sure we track executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'experiment_name' (str)\n",
      "Stored 'trial_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store experiment_name\n",
    "%store trial_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Preprocessing and Feature Engineering with Amazon SageMaker Processing</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing and feature engineering code is implemented in the `source_dir/preprocessor.py` file.\n",
    "\n",
    "You can go through the code and see that a few categorical columns required one-hot encoding, plus we are filling some NaN values based on domain knowledge.\n",
    "Once the SKLearn fit() and transform() is done, we are splitting our dataset into 80/20 train & validation and then saving to the output paths whose content will be automatically uploaded to Amazon S3 by SageMaker Processing. Finally, we also save the featurizer model as it will be reused later for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mwarnings\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\n",
      "subprocess.call([\u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33msagemaker-experiments\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtarfile\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msmexperiments\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtracker\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Tracker\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mexternals\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m joblib\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodel_selection\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m train_test_split\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpreprocessing\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m OneHotEncoder, LabelEncoder\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mcompose\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m make_column_transformer\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mexceptions\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DataConversionWarning\n",
      "warnings.filterwarnings(action=\u001b[33m'\u001b[39;49;00m\u001b[33mignore\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, category=DataConversionWarning)\n",
      "\n",
      "columns = [\u001b[33m'\u001b[39;49;00m\u001b[33mturbine_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mturbine_type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mwind_speed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mrpm_blade\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33moil_temperature\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "           \u001b[33m'\u001b[39;49;00m\u001b[33moil_level\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtemperature\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mhumidity\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mvibrations_frequency\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpressure\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mwind_direction\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mbreakdown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m==\u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    \n",
      "    \u001b[37m# Read the arguments passed to the script.\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser()\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train-test-split-ratio\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.3\u001b[39;49;00m)\n",
      "    args, _ = parser.parse_known_args()\n",
      "    \n",
      "    \u001b[37m# Tracking specific parameter value during job.\u001b[39;49;00m\n",
      "    tracker = Tracker.load()\n",
      "    tracker.log_parameter(\u001b[33m'\u001b[39;49;00m\u001b[33mtrain-test-split-ratio\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, args.train_test_split_ratio)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mReceived arguments \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args))\n",
      "\n",
      "    \u001b[37m# Read input data into a Pandas dataframe.\u001b[39;49;00m\n",
      "    input_data_path = os.path.join(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/input\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mwindturbine_raw_data_header.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mReading input data from \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(input_data_path))\n",
      "    df = pd.read_csv(input_data_path)\n",
      "    df.columns = columns\n",
      "    \n",
      "    \u001b[37m# Replacing certain null values.\u001b[39;49;00m\n",
      "    df[\u001b[33m'\u001b[39;49;00m\u001b[33mturbine_type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = df[\u001b[33m'\u001b[39;49;00m\u001b[33mturbine_type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].fillna(\u001b[33m\"\u001b[39;49;00m\u001b[33mHAWT\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    tracker.log_parameter(\u001b[33m'\u001b[39;49;00m\u001b[33mdefault-turbine-type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mHAWT\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    df[\u001b[33m'\u001b[39;49;00m\u001b[33moil_temperature\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = df[\u001b[33m'\u001b[39;49;00m\u001b[33moil_temperature\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].fillna(\u001b[34m37.0\u001b[39;49;00m)\n",
      "    tracker.log_parameter(\u001b[33m'\u001b[39;49;00m\u001b[33mdefault-oil-temperature\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[34m37.0\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# Defining one-hot encoders.\u001b[39;49;00m\n",
      "    transformer = make_column_transformer(\n",
      "        ([\u001b[33m'\u001b[39;49;00m\u001b[33mturbine_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mturbine_type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mwind_direction\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], OneHotEncoder(sparse=\u001b[34mFalse\u001b[39;49;00m)), remainder=\u001b[33m\"\u001b[39;49;00m\u001b[33mpassthrough\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    )\n",
      "    \n",
      "    X = df.drop(\u001b[33m'\u001b[39;49;00m\u001b[33mbreakdown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, axis=\u001b[34m1\u001b[39;49;00m)\n",
      "    y = df[\u001b[33m'\u001b[39;49;00m\u001b[33mbreakdown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "    \n",
      "    featurizer_model = transformer.fit(X)\n",
      "    features = featurizer_model.transform(X)\n",
      "    labels = LabelEncoder().fit_transform(y)\n",
      "    \n",
      "    \u001b[37m# Splitting.\u001b[39;49;00m\n",
      "    split_ratio = args.train_test_split_ratio\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSplitting data into train and validation sets with ratio \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(split_ratio))\n",
      "    X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=split_ratio, random_state=\u001b[34m0\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mTrain features shape after preprocessing: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(X_train.shape))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mValidation features shape after preprocessing: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(X_val.shape))\n",
      "    \n",
      "    \u001b[37m# Saving outputs.\u001b[39;49;00m\n",
      "    train_features_output_path = os.path.join(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain_features.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    train_labels_output_path = os.path.join(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain_labels.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    val_features_output_path = os.path.join(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/val\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mval_features.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    val_labels_output_path = os.path.join(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/val\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mval_labels.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSaving training features to \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_features_output_path))\n",
      "    pd.DataFrame(X_train).to_csv(train_features_output_path, header=\u001b[34mFalse\u001b[39;49;00m, index=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSaving validation features to \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(val_features_output_path))\n",
      "    pd.DataFrame(X_val).to_csv(val_features_output_path, header=\u001b[34mFalse\u001b[39;49;00m, index=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSaving training labels to \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_labels_output_path))\n",
      "    pd.DataFrame(y_train).to_csv(train_labels_output_path, header=\u001b[34mFalse\u001b[39;49;00m, index=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSaving validation labels to \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(val_labels_output_path))\n",
      "    pd.DataFrame(y_val).to_csv(val_labels_output_path, header=\u001b[34mFalse\u001b[39;49;00m, index=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# Saving model.\u001b[39;49;00m\n",
      "    model_path = os.path.join(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.joblib\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    model_output_path = os.path.join(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.tar.gz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSaving featurizer model to \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(model_output_path))\n",
      "    joblib.dump(featurizer_model, model_path)\n",
      "    tar = tarfile.open(model_output_path, \u001b[33m\"\u001b[39;49;00m\u001b[33mw:gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    tar.add(model_path, arcname=\u001b[33m\"\u001b[39;49;00m\u001b[33mmodel.joblib\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    tar.close()\n",
      "    \n",
      "    tracker.close()\n"
     ]
    }
   ],
   "source": [
    "!pygmentize source_dir/preprocessor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuring an Amazon SageMaker Processing job through the SM Python SDK requires to create a `Processor` object (in this case `SKLearnProcessor` as we are using the default SKLearn container for processing); we can specify how many instances we are going to use and what instance type is requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(role=role,\n",
    "                                     base_job_name='end-to-end-ml-sm-proc',\n",
    "                                     instance_type='ml.m5.large',\n",
    "                                     instance_count=1,\n",
    "                                     framework_version='0.20.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can invoke the `run()` method of the `Processor` object to kick-off the job, specifying the script to execute, its arguments and the configuration of inputs and outputs as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name end-to-end-ml-sm-proc-2021-02-03-07-20-29-282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  end-to-end-ml-sm-proc-2021-02-03-07-20-29-282\n",
      "Inputs:  [{'InputName': 'raw_data', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-041631420165/endtoendmlsm/data/raw/', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-041631420165/end-to-end-ml-sm-proc-2021-02-03-07-20-29-282/input/code/preprocessor.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'train_data', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-041631420165/endtoendmlsm/data/preprocessed/train/', 'LocalPath': '/opt/ml/processing/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'val_data', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-041631420165/endtoendmlsm/data/preprocessed/val/', 'LocalPath': '/opt/ml/processing/val', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'model', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-041631420165/endtoendmlsm/output/sklearn/', 'LocalPath': '/opt/ml/processing/model', 'S3UploadMode': 'EndOfJob'}}]\n",
      "..........................\u001b[34mCollecting sagemaker-experiments\n",
      "  Downloading sagemaker_experiments-0.1.26-py3-none-any.whl (41 kB)\u001b[0m\n",
      "\u001b[34mCollecting boto3>=1.16.27\n",
      "  Downloading boto3-1.17.0-py2.py3-none-any.whl (130 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /miniconda3/lib/python3.7/site-packages (from boto3>=1.16.27->sagemaker-experiments) (0.3.3)\u001b[0m\n",
      "\u001b[34mCollecting botocore<1.21.0,>=1.20.0\n",
      "  Downloading botocore-1.20.0-py2.py3-none-any.whl (7.2 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /miniconda3/lib/python3.7/site-packages (from boto3>=1.16.27->sagemaker-experiments) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.25.4 in /miniconda3/lib/python3.7/site-packages (from botocore<1.21.0,>=1.20.0->boto3>=1.16.27->sagemaker-experiments) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /miniconda3/lib/python3.7/site-packages (from botocore<1.21.0,>=1.20.0->boto3>=1.16.27->sagemaker-experiments) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /miniconda3/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.0->boto3>=1.16.27->sagemaker-experiments) (1.15.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: botocore, boto3, sagemaker-experiments\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.19.5\n",
      "    Uninstalling botocore-1.19.5:\n",
      "      Successfully uninstalled botocore-1.19.5\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.16.5\n",
      "    Uninstalling boto3-1.16.5:\n",
      "      Successfully uninstalled boto3-1.16.5\u001b[0m\n",
      "\u001b[34mSuccessfully installed boto3-1.17.0 botocore-1.20.0 sagemaker-experiments-0.1.26\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34mReceived arguments Namespace(train_test_split_ratio=0.2)\u001b[0m\n",
      "\u001b[34mReading input data from /opt/ml/processing/input/windturbine_raw_data_header.csv\u001b[0m\n",
      "\u001b[34mSplitting data into train and validation sets with ratio 0.2\u001b[0m\n",
      "\u001b[34mTrain features shape after preprocessing: (800000, 28)\u001b[0m\n",
      "\u001b[34mValidation features shape after preprocessing: (200000, 28)\u001b[0m\n",
      "\u001b[34mSaving training features to /opt/ml/processing/train/train_features.csv\u001b[0m\n",
      "\u001b[34mSaving validation features to /opt/ml/processing/val/val_features.csv\u001b[0m\n",
      "\u001b[34mSaving training labels to /opt/ml/processing/train/train_labels.csv\u001b[0m\n",
      "\u001b[34mSaving validation labels to /opt/ml/processing/val/val_labels.csv\u001b[0m\n",
      "\u001b[34mSaving featurizer model to /opt/ml/processing/model/model.tar.gz\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data_path = 's3://{0}/{1}/data/raw/'.format(bucket_name, prefix)\n",
    "train_data_path = 's3://{0}/{1}/data/preprocessed/train/'.format(bucket_name, prefix)\n",
    "val_data_path = 's3://{0}/{1}/data/preprocessed/val/'.format(bucket_name, prefix)\n",
    "model_path = 's3://{0}/{1}/output/sklearn/'.format(bucket_name, prefix)\n",
    "\n",
    "# Experiment tracking configuration\n",
    "experiment_config={\n",
    "    \"ExperimentName\": current_experiment.experiment_name,\n",
    "    \"TrialName\": current_trial.trial_name,\n",
    "    \"TrialComponentDisplayName\": \"sklearn-preprocessing\",\n",
    "}\n",
    "\n",
    "sklearn_processor.run(code='source_dir/preprocessor.py',\n",
    "                      inputs=[ProcessingInput(input_name='raw_data', source=raw_data_path, destination='/opt/ml/processing/input')],\n",
    "                      outputs=[ProcessingOutput(output_name='train_data', source='/opt/ml/processing/train', destination=train_data_path),\n",
    "                               ProcessingOutput(output_name='val_data', source='/opt/ml/processing/val', destination=val_data_path),\n",
    "                               ProcessingOutput(output_name='model', source='/opt/ml/processing/model', destination=model_path)],\n",
    "                      arguments=['--train-test-split-ratio', '0.2'],\n",
    "                      experiment_config=experiment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the job is completed, we can give a look at the preprocessed dataset, by loading the validation features as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'val_features.csv'\n",
    "s3_key_prefix = '{0}/data/preprocessed/val/{1}'.format(prefix, file_name)\n",
    "\n",
    "sagemaker_session.download_data('./', bucket_name, s3_key_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0.1</th>\n",
       "      <th>0.0.2</th>\n",
       "      <th>0.0.3</th>\n",
       "      <th>1.0</th>\n",
       "      <th>0.0.4</th>\n",
       "      <th>0.0.5</th>\n",
       "      <th>0.0.6</th>\n",
       "      <th>0.0.7</th>\n",
       "      <th>0.0.8</th>\n",
       "      <th>...</th>\n",
       "      <th>0.0.15</th>\n",
       "      <th>0.0.16</th>\n",
       "      <th>25.0</th>\n",
       "      <th>30.0</th>\n",
       "      <th>30.0.1</th>\n",
       "      <th>17.0</th>\n",
       "      <th>38.0</th>\n",
       "      <th>16.0</th>\n",
       "      <th>1.0.3</th>\n",
       "      <th>20.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0.0  0.0.1  0.0.2  0.0.3  1.0  0.0.4  0.0.5  0.0.6  0.0.7  0.0.8  ...  \\\n",
       "0  0.0    1.0    0.0    0.0  0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "1  0.0    0.0    0.0    0.0  1.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "2  0.0    0.0    0.0    0.0  0.0    0.0    0.0    1.0    0.0    0.0  ...   \n",
       "3  0.0    0.0    0.0    0.0  0.0    0.0    0.0    1.0    0.0    0.0  ...   \n",
       "4  0.0    0.0    0.0    0.0  0.0    0.0    0.0    0.0    1.0    0.0  ...   \n",
       "5  0.0    0.0    0.0    0.0  0.0    0.0    1.0    0.0    0.0    0.0  ...   \n",
       "6  0.0    0.0    1.0    0.0  0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "7  0.0    0.0    0.0    0.0  0.0    0.0    0.0    0.0    1.0    0.0  ...   \n",
       "8  0.0    0.0    0.0    0.0  1.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "9  0.0    0.0    0.0    0.0  0.0    0.0    0.0    0.0    0.0    1.0  ...   \n",
       "\n",
       "   0.0.15  0.0.16  25.0  30.0  30.0.1  17.0  38.0  16.0  1.0.3  20.0  \n",
       "0     0.0     0.0  37.0  43.0    41.0  26.0  65.0  71.0   10.0  76.0  \n",
       "1     0.0     0.0  22.0  35.0    35.0  22.0  85.0  52.0    3.0  22.0  \n",
       "2     0.0     0.0  66.0  41.0    37.0  31.0  37.0  59.0    2.0  30.0  \n",
       "3     0.0     0.0  19.0  48.0    49.0   7.0  84.0  38.0    3.0  51.0  \n",
       "4     0.0     0.0  65.0  65.0    48.0  17.0  27.0  33.0    2.0  40.0  \n",
       "5     1.0     0.0  63.0  26.0    26.0  10.0  50.0  78.0   13.0  59.0  \n",
       "6     0.0     0.0  79.0  46.0    35.0  11.0  40.0  38.0    2.0  59.0  \n",
       "7     0.0     0.0  16.0  48.0    45.0  16.0  17.0  21.0    2.0  49.0  \n",
       "8     0.0     0.0  83.0  53.0    38.0  20.0  81.0  46.0    5.0  62.0  \n",
       "9     0.0     0.0  38.0  83.0    30.0  35.0  43.0  77.0    4.0  34.0  \n",
       "\n",
       "[10 rows x 28 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the categorical variables have been one-hot encoded, and you are free to check that we do not have NaN values anymore as expected.\n",
    "Note that exploring the dataset locally with Pandas vs using Amazon Athena is possible given the limited size of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment analytics\n",
    "\n",
    "You can visualize experiment analytics either from Amazon SageMaker Studio Experiments plug-in or using the SDK from a notebook, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TrialComponentName</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>SourceArn</th>\n",
       "      <th>SageMaker.InstanceCount</th>\n",
       "      <th>SageMaker.InstanceType</th>\n",
       "      <th>SageMaker.VolumeSizeInGB</th>\n",
       "      <th>default-oil-temperature</th>\n",
       "      <th>default-turbine-type</th>\n",
       "      <th>train-test-split-ratio</th>\n",
       "      <th>SageMaker.ImageUri - MediaType</th>\n",
       "      <th>...</th>\n",
       "      <th>raw_data - MediaType</th>\n",
       "      <th>raw_data - Value</th>\n",
       "      <th>model - MediaType</th>\n",
       "      <th>model - Value</th>\n",
       "      <th>train_data - MediaType</th>\n",
       "      <th>train_data - Value</th>\n",
       "      <th>val_data - MediaType</th>\n",
       "      <th>val_data - Value</th>\n",
       "      <th>Trials</th>\n",
       "      <th>Experiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>end-to-end-ml-sm-proc-2021-02-03-07-20-29-282-...</td>\n",
       "      <td>sklearn-preprocessing</td>\n",
       "      <td>arn:aws:sagemaker:us-east-1:041631420165:proce...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ml.m5.large</td>\n",
       "      <td>30.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>HAWT</td>\n",
       "      <td>0.2</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>s3://sagemaker-us-east-1-041631420165/endtoend...</td>\n",
       "      <td>None</td>\n",
       "      <td>s3://sagemaker-us-east-1-041631420165/endtoend...</td>\n",
       "      <td>None</td>\n",
       "      <td>s3://sagemaker-us-east-1-041631420165/endtoend...</td>\n",
       "      <td>None</td>\n",
       "      <td>s3://sagemaker-us-east-1-041631420165/endtoend...</td>\n",
       "      <td>[sklearn-xgboost-1612336478]</td>\n",
       "      <td>[end-to-end-ml-sagemaker-1612336463]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  TrialComponentName            DisplayName  \\\n",
       "0  end-to-end-ml-sm-proc-2021-02-03-07-20-29-282-...  sklearn-preprocessing   \n",
       "\n",
       "                                           SourceArn  SageMaker.InstanceCount  \\\n",
       "0  arn:aws:sagemaker:us-east-1:041631420165:proce...                      1.0   \n",
       "\n",
       "  SageMaker.InstanceType  SageMaker.VolumeSizeInGB  default-oil-temperature  \\\n",
       "0            ml.m5.large                      30.0                     37.0   \n",
       "\n",
       "  default-turbine-type  train-test-split-ratio SageMaker.ImageUri - MediaType  \\\n",
       "0                 HAWT                     0.2                           None   \n",
       "\n",
       "   ... raw_data - MediaType  \\\n",
       "0  ...                 None   \n",
       "\n",
       "                                    raw_data - Value model - MediaType  \\\n",
       "0  s3://sagemaker-us-east-1-041631420165/endtoend...              None   \n",
       "\n",
       "                                       model - Value train_data - MediaType  \\\n",
       "0  s3://sagemaker-us-east-1-041631420165/endtoend...                   None   \n",
       "\n",
       "                                  train_data - Value val_data - MediaType  \\\n",
       "0  s3://sagemaker-us-east-1-041631420165/endtoend...                 None   \n",
       "\n",
       "                                    val_data - Value  \\\n",
       "0  s3://sagemaker-us-east-1-041631420165/endtoend...   \n",
       "\n",
       "                         Trials                           Experiments  \n",
       "0  [sklearn-xgboost-1612336478]  [end-to-end-ml-sagemaker-1612336463]  \n",
       "\n",
       "[1 rows x 23 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "\n",
    "analytics = ExperimentAnalytics(experiment_name=experiment_name)\n",
    "analytics.dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the preprocessing and feature engineering are completed, you can move to the next notebook in the **03_train_model** folder to start model training."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
