{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Pipeline Model Deployment</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have built and trained our models for feature engineering (using Amazon SageMaker Processing and SKLearn) and binary classification (using the XGBoost open-source container for Amazon SageMaker), we can choose to deploy them in a pipeline on Amazon SageMaker Hosting, by creating an Inference Pipeline.\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipelines.html\n",
    "\n",
    "This notebook demonstrates how to create a pipeline with the SKLearn model for feature engineering and the XGBoost model for binary classification.\n",
    "\n",
    "Let's define the variables first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When using Amazon SageMaker Studio, please set this variable to True and execute the cell\n",
    "use_sm_studio = True\n",
    "if use_sm_studio:\n",
    "    %cd /root/end-to-end-ml-sm/04_deploy_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check SageMaker Python SDK version\n",
    "import sagemaker\n",
    "print(sagemaker.__version__)\n",
    "\n",
    "def versiontuple(v):\n",
    "    return tuple(map(int, (v.split(\".\"))))\n",
    "\n",
    "if versiontuple(sagemaker.__version__) < versiontuple('2.5.0'):\n",
    "    raise Exception(\"This notebook requires at least SageMaker Python SDK version 2.5.0. Please install it via pip.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "prefix = 'endtoendmlsm'\n",
    "\n",
    "print(region)\n",
    "print(role)\n",
    "print(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve model artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to create two Amazon SageMaker **Model** objects, which associate the artifacts of training (serialized model artifacts in Amazon S3) to the Docker container used for inference. In order to do that, we need to get the paths to our serialized models in Amazon S3.\n",
    "<ul>\n",
    "    <li>For the SKLearn model, in Step 02 (data exploration and feature engineering) we defined the path where the artifacts are saved</li>\n",
    "    <li>For the XGBoost model, we need to find the path based on Amazon SageMaker's naming convention. We are going to use a utility function to get the model artifacts of the last training job matching a specific base job name.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utilities import get_latest_training_job_name, get_training_job_s3_model_artifacts\n",
    "\n",
    "# SKLearn model artifacts path.\n",
    "sklearn_model_path = 's3://{0}/{1}/output/sklearn/model.tar.gz'.format(bucket_name, prefix)\n",
    "\n",
    "# XGBoost model artifacts path.\n",
    "training_base_job_name = 'end-to-end-ml-sm-xgb'\n",
    "latest_training_job_name = get_latest_training_job_name(training_base_job_name)\n",
    "xgboost_model_path = get_training_job_s3_model_artifacts(latest_training_job_name)\n",
    "\n",
    "print('SKLearn model path: ' + sklearn_model_path)\n",
    "print('XGBoost model path: ' + xgboost_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLearn Featurizer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the SKLearn model. For hosting this model we also provide a custom inference script, that is used to process the inputs and outputs and execute the transform.\n",
    "\n",
    "The inference script is implemented in the `sklearn_source_dir/inference.py` file. The custom script defines:\n",
    "\n",
    "- a custom `input_fn` for pre-processing inference requests. Our input function accepts only CSV input, loads the input in a Pandas dataframe and assigns feature column names to the dataframe\n",
    "- a custom `predict_fn` for running the transform over the inputs\n",
    "- a custom `output_fn` for returning either JSON or CSV\n",
    "- a custom `model_fn` for deserializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize sklearn_source_dir/inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create the `SKLearnModel` object, by providing the custom script and S3 model artifacts as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.sklearn import SKLearnModel\n",
    "\n",
    "code_location = 's3://{0}/{1}/code'.format(bucket_name, prefix)\n",
    "\n",
    "sklearn_model = SKLearnModel(name='end-to-end-ml-sm-skl-model-{0}'.format(str(int(time.time()))),\n",
    "                             model_data=sklearn_model_path,\n",
    "                             entry_point='inference.py',\n",
    "                             source_dir='sklearn_source_dir/',\n",
    "                             code_location=code_location,\n",
    "                             role=role,\n",
    "                             sagemaker_session=sagemaker_session,\n",
    "                             framework_version='0.20.0',\n",
    "                             py_version='py3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the previous steps, we can create an `XGBoost` model object. Also here, we have to provide a custom inference script.\n",
    "\n",
    "The inference script is implemented in the `xgboost_source_dir/inference.py` file. The custom script defines:\n",
    "\n",
    "- a custom `input_fn` for pre-processing inference requests. This input function is able to handle JSON requests, plus all content types supported by the default XGBoost container. For additional information please visit: https://github.com/aws/sagemaker-xgboost-container/blob/master/src/sagemaker_xgboost_container/encoder.py. The reason for adding the JSON content type is that the container-to-container default request content type in an inference pipeline is JSON.\n",
    "- a custom `model_fn` for deserializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize xgboost_source_dir/inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create the `XGBoostModel` object, by providing the custom script and S3 model artifacts as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.xgboost import XGBoostModel\n",
    "\n",
    "code_location = 's3://{0}/{1}/code'.format(bucket_name, prefix)\n",
    "\n",
    "xgboost_model = XGBoostModel(name='end-to-end-ml-sm-xgb-model-{0}'.format(str(int(time.time()))),\n",
    "                             model_data=xgboost_model_path,\n",
    "                             entry_point='inference.py',\n",
    "                             source_dir='xgboost_source_dir/',\n",
    "                             code_location=code_location,\n",
    "                             framework_version='0.90-2',\n",
    "                             py_version='py3',\n",
    "                             role=role, \n",
    "                             sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have models ready, we can deploy them in a pipeline, by building a `PipelineModel` object and calling the `deploy()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we create an endpoint with data capture enabled, for monitoring the model data quality. Data capture is enabled at enpoint configuration level for the Amazon SageMaker real-time endpoint. You can choose to capture the request payload, the response payload or both and captured data is stored in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import time\n",
    "from sagemaker.pipeline import PipelineModel\n",
    "from time import gmtime, strftime\n",
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "s3_capture_upload_path = 's3://{}/{}/monitoring/datacapture'.format(bucket_name, prefix)\n",
    "print(s3_capture_upload_path)\n",
    "\n",
    "pipeline_model_name = 'end-to-end-ml-sm-xgb-skl-pipeline-{0}'.format(str(int(time.time())))\n",
    "\n",
    "pipeline_model = PipelineModel(\n",
    "    name=pipeline_model_name, \n",
    "    role=role,\n",
    "    models=[\n",
    "        sklearn_model, \n",
    "        xgboost_model],\n",
    "    sagemaker_session=sagemaker_session)\n",
    "\n",
    "endpoint_name = 'end-to-end-ml-sm-pipeline-endpoint-{0}'.format(str(int(time.time())))\n",
    "print(endpoint_name)\n",
    "\n",
    "pipeline_model.deploy(initial_instance_count=1, \n",
    "                      instance_type='ml.m5.xlarge', \n",
    "                      endpoint_name=endpoint_name,\n",
    "                      data_capture_config=DataCaptureConfig(\n",
    "                          enable_capture=True,\n",
    "                          sampling_percentage=100,\n",
    "                          destination_s3_uri=s3_capture_upload_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red; font-weight:bold\">Please take note of the endpoint name, since it will be used in the next workshop module.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting inferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can try invoking our pipeline of models and get some inferences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import CSVDeserializer\n",
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    serializer=CSVSerializer(),\n",
    "    deserializer=CSVDeserializer())\n",
    "\n",
    "payload = \"TID008,HAWT,64,80,46,21,55,55,7,34,SE\"\n",
    "print(predictor.predict(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now let's list the data capture files stored in S3. You should expect to see different files from different time periods organized based on the hour in which the invocation occurred.\n",
    "\n",
    "Note that the delivery of capture data to Amazon S3 can require a couple of minutes so next cell might error. If this happens, please retry after a minute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.Session().client('s3')\n",
    "current_endpoint_capture_prefix = '{}/monitoring/datacapture/{}'.format(prefix, endpoint_name)\n",
    "\n",
    "result = s3_client.list_objects(Bucket=bucket_name, Prefix=current_endpoint_capture_prefix)\n",
    "capture_files = ['s3://{0}/{1}'.format(bucket_name, capture_file.get(\"Key\")) for capture_file in result.get('Contents')]\n",
    "\n",
    "print(\"Capture Files: \")\n",
    "print(\"\\n \".join(capture_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also read the contents of one of these files and see how capture records are organized in JSON lines format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp {capture_files[0]} datacapture/captured_data_example.jsonl\n",
    "!head datacapture/captured_data_example.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we can better understand the content of each JSON line like follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open (\"datacapture/captured_data_example.jsonl\", \"r\") as myfile:\n",
    "    data=myfile.read()\n",
    "\n",
    "print(json.dumps(json.loads(data.split('\\n')[0]), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "For each inference request, we get input data, output data and some metadata like the inference time captured and saved.\n",
    "## Baselining\n",
    "\n",
    "From our validation dataset let's ask Amazon SageMaker to suggest a set of baseline constraints and generate descriptive statistics for our features. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_data_path = 's3://{0}/{1}/data/raw'.format(bucket_name, prefix)\n",
    "baseline_results_path = 's3://{0}/{1}/monitoring/baselining/results'.format(bucket_name, prefix)\n",
    "\n",
    "print(baseline_data_path)\n",
    "print(baseline_results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that running the baselining job will require 8-10 minutes. In the meantime, you can take a look at the Deequ library, used to execute these analyses with the default Model Monitor container: https://github.com/awslabs/deequ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "my_default_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c5.4xlarge',\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_default_monitor.suggest_baseline(\n",
    "    baseline_dataset=baseline_data_path,\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=baseline_results_path,\n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let's display the statistics that were generated by the baselining job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "baseline_job = my_default_monitor.latest_baselining_job\n",
    "schema_df = pd.json_normalize(baseline_job.baseline_statistics().body_dict[\"features\"])\n",
    "schema_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can also visualize the constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_df = pd.json_normalize(baseline_job.suggested_constraints().body_dict[\"features\"])\n",
    "constraints_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switching order of target variable\n",
    "\n",
    "Amazon SageMaker Model Monitor expects that the target variable is the first feature of your dataset when comparing CSV captured data with the baseline.\n",
    "However, since the dataset we used for baselining had the breakdown variable as last feature, we are going to switch its order in the generated statistics and constraints file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_path = baseline_results_path + '/statistics.json'\n",
    "constraints_path = baseline_results_path + '/constraints.json'\n",
    "\n",
    "!aws s3 cp {statistics_path} baseline/\n",
    "!aws s3 cp {constraints_path} baseline/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('baseline/statistics.json', 'r') as statistics_file:\n",
    "    loaded_statistics = json.load(statistics_file)\n",
    "\n",
    "loaded_statistics['features'].insert(0, loaded_statistics['features'][-1])\n",
    "del loaded_statistics['features'][-1]\n",
    "\n",
    "with open('baseline/statistics.json', 'w') as statistics_file:\n",
    "    json.dump(loaded_statistics, statistics_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp baseline/statistics.json {statistics_path} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('baseline/constraints.json', 'r') as constraints_file:\n",
    "    loaded_constraints = json.load(constraints_file)\n",
    "\n",
    "loaded_constraints['features'].insert(0, loaded_constraints['features'][-1])\n",
    "del loaded_constraints['features'][-1]\n",
    "\n",
    "with open('baseline/constraints.json', 'w') as constraints_file:\n",
    "    json.dump(loaded_constraints, constraints_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp baseline/constraints.json {constraints_path} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The baselining job has inspected the validation dataset and generated constraints and statistics, that will be used to monitor our endpoint.\n",
    "## Generating violations artificially\n",
    "\n",
    "In order to get some result relevant to monitoring analysis, you can try and generate artificially some inferences with feature values causing specific violations, and then invoke the endpoint with this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "dist_values = np.random.normal(1, 0.2, 200)\n",
    "\n",
    "# wind_speed -> set to float (expected integer)\n",
    "# rpm_blade -> set to empty (missing value)\n",
    "# humidity -> sampled from random normal distribution [seventh feature]\n",
    "#TODO\n",
    "artificial_values = \"TID008,HAWT,65.8,,46,21,55,{0},7,34,SE\"\n",
    "\n",
    "for i in range(200):\n",
    "    predictor.predict(artificial_values.format(str(dist_values[i])))\n",
    "    time.sleep(0.15)\n",
    "    if i > 0 and i % 100 == 0 :\n",
    "        print('Executed {0} inferences.'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Monitoring\n",
    "\n",
    "Once we have built the baseline for our data, we can enable endpoint monitoring by creating a monitoring schedule. When the schedule fires, a monitoring job will be kicked-off and will inspect the data captured at the endpoint with respect to the baseline; then it will generate some report files that can be used to analyze monitoring results.\n",
    "## Create Monitoring Schedule\n",
    "\n",
    "Let's create the monitoring schedule for the previously created endpoint. When we create the schedule, we can also specify two scripts that will preprocess the records before the analysis takes place and execute post-processing at the end. For this example, we are not going to use a record preprocessor, and we are just specifying a post-processor that outputs some text for demo purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize postprocessor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "monitoring_code_prefix = '{0}/monitoring/code'.format(prefix)\n",
    "print(monitoring_code_prefix)\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(monitoring_code_prefix + '/postprocessor.py').upload_file('postprocessor.py')\n",
    "postprocessor_path = 's3://{0}/{1}/monitoring/code/postprocessor.py'.format(bucket_name, prefix)\n",
    "print(postprocessor_path)\n",
    "\n",
    "reports_path = 's3://{0}/{1}/monitoring/reports'.format(bucket_name, prefix)\n",
    "print(reports_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create the monitoring schedule with hourly schedule execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "from time import gmtime, strftime\n",
    "\n",
    "endpoint_name = predictor.endpoint_name\n",
    "\n",
    "mon_schedule_name = 'end-to-end-ml-sm-mon-sch-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "my_default_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=mon_schedule_name,\n",
    "    endpoint_input=endpoint_name,\n",
    "    post_analytics_processor_script=postprocessor_path,\n",
    "    output_s3_uri=reports_path,\n",
    "    statistics=my_default_monitor.baseline_statistics(),\n",
    "    constraints=my_default_monitor.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    enable_cloudwatch_metrics=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe Monitoring Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_schedule_result = my_default_monitor.describe_schedule()\n",
    "desc_schedule_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Delete Monitoring Schedule\n",
    "\n",
    "Once the schedule is created, it will kick of jobs at specified intervals. Note that if you are kicking this off after creating the hourly schedule, you might find the executions empty. You might have to wait till you cross the hour boundary (in UTC) to see executions kick off. Since we don't want to wait for the hour in this example we can delete the schedule and use the code in next steps to simulate what will happen when a schedule is triggered, by running an Amazon SageMaker Processing Job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this is just for the purpose of running this example.\n",
    "my_default_monitor.delete_monitoring_schedule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triggering execution manually\n",
    "\n",
    "In oder to trigger the execution manually, we first get all paths to data capture, baseline statistics, baseline constraints, etc. Then, we use a utility fuction, defined in monitoringjob_utils.py, to run the processing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = s3_client.list_objects(Bucket=bucket_name, Prefix=current_endpoint_capture_prefix)\n",
    "capture_files = ['s3://{0}/{1}'.format(bucket_name, capture_file.get(\"Key\")) for capture_file in result.get('Contents')]\n",
    "\n",
    "print(\"Capture Files: \")\n",
    "print(\"\\n \".join(capture_files))\n",
    "\n",
    "data_capture_path = capture_files[len(capture_files) - 1][: capture_files[len(capture_files) - 1].rfind('/')]\n",
    "statistics_path = baseline_results_path + '/statistics.json'\n",
    "constraints_path = baseline_results_path + '/constraints.json'\n",
    "\n",
    "print(data_capture_path)\n",
    "print(postprocessor_path)\n",
    "print(statistics_path)\n",
    "print(constraints_path)\n",
    "print(reports_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monitoringjob_utils import run_model_monitor_job_processor\n",
    "\n",
    "run_model_monitor_job_processor(region, 'ml.m5.xlarge', role, data_capture_path, statistics_path, constraints_path, reports_path,\n",
    "                                postprocessor_path=postprocessor_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "When the monitoring job completes, monitoring reports are saved to Amazon S3. Let's list the generated reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.Session().client('s3')\n",
    "monitoring_reports_prefix = '{}/monitoring/reports/{}'.format(prefix, predictor.endpoint_name)\n",
    "\n",
    "result = s3_client.list_objects(Bucket=bucket_name, Prefix=monitoring_reports_prefix)\n",
    "try:\n",
    "    monitoring_reports = ['s3://{0}/{1}'.format(bucket_name, capture_file.get(\"Key\")) for capture_file in result.get('Contents')]\n",
    "    print(\"Monitoring Reports Files: \")\n",
    "    print(\"\\n \".join(monitoring_reports))\n",
    "except:\n",
    "    print('No monitoring reports found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp {monitoring_reports[0]} monitoring/\n",
    "!aws s3 cp {monitoring_reports[1]} monitoring/\n",
    "!aws s3 cp {monitoring_reports[2]} monitoring/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the violations identified by the monitoring execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "file = open('monitoring/constraint_violations.json', 'r')\n",
    "data = file.read()\n",
    "\n",
    "violations_df = pd.json_normalize(json.loads(data)['violations'])\n",
    "violations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Advanced Hints\n",
    "\n",
    "You might be asking yourself what are the type of violations that are monitored and how drift from the baseline is computed.\n",
    "\n",
    "The types of violations monitored are listed here: https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-interpreting-violations.html. Most of them use configurable thresholds, that are specified in the monitoring configuration section of the baseline constraints JSON. Let's take a look at this configuration from the baseline constraints file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp {statistics_path} baseline/\n",
    "!aws s3 cp {constraints_path} baseline/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open (\"baseline/constraints.json\", \"r\") as myfile:\n",
    "    data=myfile.read()\n",
    "\n",
    "print(json.dumps(json.loads(data)['monitoring_config'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This configuration is intepreted when the monitoring job is executed and used to compare captured data to the baseline. If you want to customize this section, you will have to update the constraints.json file and upload it back to Amazon S3 before launching the monitoring job.\n",
    "\n",
    "When data distributions are compared to detect potential drift, you can choose to use either a Simple or Robust comparison method, where the latter has to be preferred when dealing with small datasets. Additional info: https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-byoc-constraints.html.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictor.delete_endpoint()\n",
    "#predictor.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have tested the endpoint, we can move to the next workshop module. Please access the module <a href=\"https://github.com/giuseppeporcelli/end-to-end-ml-sm/tree/master/05_API_Gateway_and_Lambda\" target=\"_blank\">05_API_Gateway_and_Lambda</a> on GitHub to continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
