{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model Build Workflow</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will show how to define a model build workflow that orchestrates the previous steps (processing, training) and registers models in the SageMaker Model Registry. We will use Amazon SageMaker Pipelines for the workflow orchestration and lineage.\n",
    "\n",
    "Orchestrating and automating the model build workflow is preliminary to any ML CI/CD, since CI/CD automations must be capable of executing the steps that lead to the generation of a model, which can vary based on the use case. The idea is that a typical \"build\" stage of CI/CD will execute a workflow that has been previously defined by a Data Scientist.\n",
    "\n",
    "Amazon SageMaker Pipelines  supports a pipeline Domain Specific Language (DSL), which is a declarative Json specification. This DSL defines a Directed Acyclic Graph (DAG) of pipeline parameters and SageMaker job steps. The SageMaker Python SDK streamlines the generation of the pipeline DSL using constructs that are already familiar to engineers and scientists alike.\n",
    "\n",
    "SageMaker Model Registry is where trained models are stored, versioned, and managed. Data Scientists and Machine Learning Engineers can compare model versions, approve models for deployment, and deploy models from different AWS accounts, all from a single Model Registry.\n",
    "\n",
    "Let's define the variables first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check SageMaker Python SDK version\n",
    "import sagemaker\n",
    "print(sagemaker.__version__)\n",
    "\n",
    "def versiontuple(v):\n",
    "    return tuple(map(int, (v.split(\".\"))))\n",
    "\n",
    "if versiontuple(sagemaker.__version__) < versiontuple('2.22.0'):\n",
    "    raise Exception(\"This notebook requires at least SageMaker Python SDK version 2.22.0. Please install it via pip.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "prefix = 'endtoendmlsm'\n",
    "\n",
    "print(region)\n",
    "print(role)\n",
    "print(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define Pipeline</h2>\n",
    "\n",
    "In this section, we will define a model build workflow for the pre-processing and training operations that we have executed manually in the previous notebooks. The workflow definition will also include steps to register models in the SageMaker model registry.\n",
    "\n",
    "Our objective is defining a pipeline as graphically shown below: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./workflow.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline will execute the following steps:\n",
    "<ul>\n",
    "    <li>Run a SM Processing job to execute data preparation and generate a featurizer model</li>\n",
    "    <ul>\n",
    "        <li>Register the featurizer model in the SM Model Registry</li>\n",
    "        <li>Run a SM Training job to train the XGBoost model</li>\n",
    "        <ul><li>Register the XGBoost model in the SM Model Registry</li></ul>\n",
    "    </ul>\n",
    "</ul>\n",
    "\n",
    "Note: the repack model steps will be automatically added by SM to convert the models in a suitable format for the SM Model Registry, when custom inference logic is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Pipeline parameters</h3>\n",
    "\n",
    "We define workflow parameters by which we can parametrize our pipeline and vary the values injected and used in pipeline executions and schedules without having to modify the definition.\n",
    "\n",
    "The supported parameter types include:\n",
    "\n",
    "* `ParameterString` - representing a `str` Python type\n",
    "* `ParameterInteger` - representing an `int` Python type\n",
    "* `ParameterFloat` - representing a `float` Python type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    ")\n",
    "\n",
    "# ---------------------\n",
    "# Processing parameters\n",
    "# ---------------------\n",
    "\n",
    "# The path to the raw data.\n",
    "raw_data_path = 's3://{0}/{1}/data/raw/'.format(bucket_name, prefix)\n",
    "raw_data_path_param = ParameterString(name=\"raw_data_path\", default_value=raw_data_path)\n",
    "\n",
    "# The output path to the training data.\n",
    "train_data_path = 's3://{0}/{1}/data/preprocessed/train/'.format(bucket_name, prefix)\n",
    "train_data_path_param = ParameterString(name=\"train_data_path\", default_value=train_data_path)\n",
    "\n",
    "# The output path to the validation data.\n",
    "val_data_path = 's3://{0}/{1}/data/preprocessed/val/'.format(bucket_name, prefix)\n",
    "val_data_path_param = ParameterString(name=\"val_data_path\", default_value=val_data_path)\n",
    "\n",
    "# The output path to the featurizer model.\n",
    "model_path = 's3://{0}/{1}/output/sklearn/'.format(bucket_name, prefix)\n",
    "model_path_param = ParameterString(name=\"model_path\", default_value=model_path)\n",
    "\n",
    "# The instance type for the processing job.\n",
    "processing_instance_type_param = ParameterString(name=\"processing_instance_type\", default_value='ml.m5.large')\n",
    "\n",
    "# The instance count for the processing job.\n",
    "processing_instance_count_param = ParameterInteger(name=\"processing_instance_count\", default_value=1)\n",
    "\n",
    "# The train/test split ration parameter.\n",
    "train_test_split_ratio_param = ParameterString(name=\"train_test_split_ratio\", default_value='0.2')\n",
    "\n",
    "# -------------------\n",
    "# Training parameters\n",
    "# -------------------\n",
    "        \n",
    "# XGB hyperparameters.\n",
    "max_depth_param = ParameterInteger(name=\"max_depth\", default_value=3)\n",
    "eta_param = ParameterFloat(name=\"eta\", default_value=0.1)\n",
    "gamma_param = ParameterInteger(name=\"gamma\", default_value=6)\n",
    "min_child_weight_param = ParameterInteger(name=\"min_child_weight\", default_value=6)\n",
    "objective_param = ParameterString(name=\"objective\", default_value='reg:logistic')\n",
    "num_round_param = ParameterInteger(name=\"num_round\", default_value=20)\n",
    "\n",
    "# The instance type for the training job.\n",
    "training_instance_type_param = ParameterString(name=\"training_instance_type\", default_value='ml.m5.xlarge')\n",
    "\n",
    "# The instance count for the training job.\n",
    "training_instance_count_param = ParameterInteger(name=\"training_instance_count\", default_value=1)\n",
    "\n",
    "# The training output path for the model.\n",
    "output_path = 's3://{0}/{1}/output/'.format(bucket_name, prefix)\n",
    "output_path_param = ParameterString(name=\"output_path\", default_value=output_path)\n",
    "\n",
    "# --------------------------\n",
    "# Register models parameters\n",
    "# --------------------------\n",
    "\n",
    "# The default intance type for deployment.\n",
    "deploy_instance_type_param = ParameterString(name=\"deploy_instance_type\", default_value='ml.m5.2xlarge')\n",
    "\n",
    "# The approval status for models added to the registry.\n",
    "model_approval_status_param = ParameterString(name=\"model_approval_status\", default_value='PendingManualApproval')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Processing Step</h3>\n",
    "\n",
    "Now, we can start by defining the processing step that will prepare our dataset, as seen in module 02_data_exploration_and_feature_eng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pygmentize ../02_data_exploration_and_feature_eng/source_dir/preprocessor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(role=role,\n",
    "                                     instance_type=processing_instance_type_param,\n",
    "                                     instance_count=processing_instance_count_param,\n",
    "                                     framework_version='0.20.0')\n",
    "\n",
    "inputs = [ProcessingInput(input_name='raw_data', \n",
    "                          source=raw_data_path_param, destination='/opt/ml/processing/input')]\n",
    "\n",
    "outputs = [ProcessingOutput(output_name='train_data', \n",
    "                            source='/opt/ml/processing/train', destination=train_data_path_param),\n",
    "           ProcessingOutput(output_name='val_data', \n",
    "                            source='/opt/ml/processing/val', destination=val_data_path_param),\n",
    "           ProcessingOutput(output_name='model', \n",
    "                            source='/opt/ml/processing/model', destination=model_path_param)]\n",
    "\n",
    "code_path = '../02_data_exploration_and_feature_eng/source_dir/preprocessor.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "processing_step = ProcessingStep(\n",
    "    name='Processing', \n",
    "    code=code_path,\n",
    "    processor=sklearn_processor,\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    job_arguments=['--train-test-split-ratio', train_test_split_ratio_param]\n",
    ")\n",
    "\n",
    "print(processing_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training Step</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pygmentize ../03_train_model/source_dir/training.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.xgboost import XGBoost\n",
    "\n",
    "hyperparameters = {\n",
    "    \"max_depth\": max_depth_param,\n",
    "    \"eta\": eta_param,\n",
    "    \"gamma\": gamma_param,\n",
    "    \"min_child_weight\": min_child_weight_param,\n",
    "    \"silent\": 0,\n",
    "    \"objective\": objective_param,\n",
    "    \"num_round\": num_round_param\n",
    "}\n",
    "\n",
    "entry_point='training.py'\n",
    "source_dir='../03_train_model/source_dir/'\n",
    "code_location = 's3://{0}/{1}/code'.format(bucket_name, prefix)\n",
    "\n",
    "estimator = XGBoost(\n",
    "    entry_point=entry_point,\n",
    "    source_dir=source_dir,\n",
    "    output_path=output_path_param,\n",
    "    code_location=code_location,\n",
    "    hyperparameters=hyperparameters,\n",
    "    instance_type=training_instance_type_param,\n",
    "    instance_count=training_instance_count_param,\n",
    "    framework_version=\"0.90-2\",\n",
    "    py_version=\"py3\",\n",
    "    role=role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "training_step = TrainingStep(\n",
    "    name='Training',\n",
    "    estimator=estimator,\n",
    "    inputs={\n",
    "        'train': TrainingInput(\n",
    "            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                'train_data'\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type='text/csv'\n",
    "        ),\n",
    "        'validation': TrainingInput(\n",
    "            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                'val_data'\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type='text/csv'\n",
    "        )      \n",
    "    },\n",
    ")\n",
    "\n",
    "print(training_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Register Model Steps</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Featurizer Model</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_group_name_featurizer = 'end-to-end-ml-sagemaker-sklearn-featurizer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework='sklearn',\n",
    "    region=region,\n",
    "    version='0.20.0',\n",
    "    py_version='py3',\n",
    "    instance_type=deploy_instance_type_param,\n",
    "    image_scope='inference'\n",
    ")\n",
    "print(inference_image_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "dummy_estimator = SKLearn(sagemaker_session=sagemaker_session,\n",
    "                          entry_point='inference.py',\n",
    "                          source_dir='../04_deploy_model/sklearn_source_dir',\n",
    "                          image_uri=inference_image_uri,\n",
    "                          role=role,\n",
    "                          instance_type=deploy_instance_type_param,\n",
    "                          instance_count=1)\n",
    "\n",
    "register_featurizer_step = RegisterModel(\n",
    "    name='RegisterFeaturizerModel',\n",
    "    estimator=dummy_estimator,\n",
    "    entry_point='inference.py',\n",
    "    source_dir='../04_deploy_model/sklearn_source_dir',\n",
    "    image_uri=inference_image_uri,\n",
    "    model_data=processing_step.properties.ProcessingOutputConfig.Outputs['model'].S3Output.S3Uri,\n",
    "    content_types=['text/csv'],\n",
    "    response_types=['application/json', 'text/csv'],\n",
    "    inference_instances=[deploy_instance_type_param],\n",
    "    transform_instances=['ml.c5.4xlarge'],\n",
    "    model_package_group_name=model_package_group_name_featurizer,\n",
    "    approval_status=model_approval_status_param\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>XGBoost Model</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_group_name_xgboost = 'end-to-end-ml-sagemaker-xgboost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_image_uri=sagemaker.image_uris.retrieve(\n",
    "    framework='xgboost',\n",
    "    region=region,\n",
    "    version='0.90-2',\n",
    "    py_version='py3',\n",
    "    instance_type=deploy_instance_type_param,\n",
    "    image_scope='inference'\n",
    ")\n",
    "print(inference_image_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_xgboost_step=RegisterModel(\n",
    "    name='RegisterXGBoostModel',\n",
    "    estimator=estimator,\n",
    "    entry_point='inference.py',\n",
    "    source_dir='../04_deploy_model/xgboost_source_dir',\n",
    "    image_uri=inference_image_uri,\n",
    "    model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=['text/csv', 'application/json'],\n",
    "    response_types=['text/csv', 'application/json'],\n",
    "    inference_instances=[deploy_instance_type_param],\n",
    "    transform_instances=['ml.c5.4xlarge'],\n",
    "    model_package_group_name=model_package_group_name_xgboost,\n",
    "    approval_status=model_approval_status_param\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Pipeline</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = 'end-to-end-ml-sagemaker-pipeline'\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        raw_data_path_param,\n",
    "        train_data_path_param,\n",
    "        val_data_path_param,\n",
    "        model_path_param,\n",
    "        processing_instance_type_param,\n",
    "        processing_instance_count_param,\n",
    "        train_test_split_ratio_param,\n",
    "        max_depth_param,\n",
    "        eta_param,\n",
    "        gamma_param,\n",
    "        min_child_weight_param,\n",
    "        objective_param,\n",
    "        num_round_param,\n",
    "        training_instance_type_param,\n",
    "        training_instance_count_param,\n",
    "        output_path_param,\n",
    "        deploy_instance_type_param,\n",
    "        model_approval_status_param\n",
    "    ],\n",
    "    steps=[processing_step, training_step, register_featurizer_step, register_xgboost_step],\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Insert and Execute the pipeline</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pipeline.upsert(role_arn=role)\n",
    "\n",
    "pipeline_arn = response[\"PipelineArn\"]\n",
    "print(pipeline_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()\n",
    "print(execution.arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Wait for pipeline execution</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "execution.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While waiting for pipeline execution to complete (it will take ~10mins), feel free to use the left side panel in SageMaker Studio to review the pipeline definition and execution status."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Approve models in registry</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = execution.list_steps()\n",
    "register_sklearn_step = next(s for s in steps if s['StepName'] == 'RegisterFeaturizerModel' )\n",
    "register_xgboost_step = next(s for s in steps if s['StepName'] == 'RegisterXGBoostModel' )\n",
    "\n",
    "sklearn_model_package_arn = register_sklearn_step['Metadata']['RegisterModel']['Arn']\n",
    "xgboost_model_package_arn = register_xgboost_step['Metadata']['RegisterModel']['Arn']\n",
    "\n",
    "print(sklearn_model_package_arn)\n",
    "print(xgboost_model_package_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "sm_client.update_model_package(\n",
    "    ModelPackageArn=sklearn_model_package_arn,\n",
    "    ModelApprovalStatus=\"Approved\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.update_model_package(\n",
    "    ModelPackageArn=xgboost_model_package_arn,\n",
    "    ModelApprovalStatus=\"Approved\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Deploy real-time endpoint from models in the registry</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_mp_response = sm_client.describe_model_package(ModelPackageName = sklearn_model_package_arn)\n",
    "xgboost_mp_response = sm_client.describe_model_package(ModelPackageName = xgboost_model_package_arn)\n",
    "\n",
    "sklearn_container = sklearn_mp_response['InferenceSpecification']['Containers'][0]['Image']\n",
    "sklearn_model_data = sklearn_mp_response['InferenceSpecification']['Containers'][0]['ModelDataUrl']\n",
    "print(sklearn_container)\n",
    "print(sklearn_model_data)\n",
    "print()\n",
    "\n",
    "xgboost_container = xgboost_mp_response['InferenceSpecification']['Containers'][0]['Image']\n",
    "xgboost_model_data = xgboost_mp_response['InferenceSpecification']['Containers'][0]['ModelDataUrl']\n",
    "print(xgboost_container)\n",
    "print(xgboost_model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_model_path = sklearn_model_data[0:sklearn_model_data.rfind('/')] + '/'\n",
    "xgboost_model_path = xgboost_model_data[0:sklearn_model_data.rfind('/')] + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -cvzf sklearn_sourcedir.tar.gz -C ../04_deploy_model/sklearn_source_dir/ .\n",
    "!aws s3 cp sklearn_sourcedir.tar.gz {sklearn_model_path}\n",
    "!tar -cvzf xgboost_sourcedir.tar.gz -C ../04_deploy_model/xgboost_source_dir/ .\n",
    "!aws s3 cp xgboost_sourcedir.tar.gz {xgboost_model_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.pipeline import PipelineModel\n",
    "\n",
    "sklearn_model = Model(image_uri = sklearn_container,\n",
    "                      model_data = sklearn_model_data,\n",
    "                      env = {\n",
    "                          'SAGEMAKER_PROGRAM' : 'inference.py',\n",
    "                          'SAGEMAKER_SUBMIT_DIRECTORY' : sklearn_model_path + 'sklearn_sourcedir.tar.gz',\n",
    "                      },\n",
    "                      role = role,\n",
    "                      sagemaker_session = sagemaker_session) \n",
    "\n",
    "xgboost_model = Model(image_uri = xgboost_container,\n",
    "                      model_data = xgboost_model_data,\n",
    "                      env = {\n",
    "                          'SAGEMAKER_PROGRAM' : 'inference.py',\n",
    "                          'SAGEMAKER_SUBMIT_DIRECTORY' : xgboost_model_path + 'xgboost_sourcedir.tar.gz',\n",
    "                      },\n",
    "                      role = role,\n",
    "                      sagemaker_session = sagemaker_session)\n",
    "\n",
    "pipeline_model_name = 'end-to-end-ml-sm-xgb-skl-pipeline-{0}'.format(str(int(time.time())))\n",
    "\n",
    "pipeline_model = PipelineModel(\n",
    "    name=pipeline_model_name, \n",
    "    role=role,\n",
    "    models=[\n",
    "        sklearn_model, \n",
    "        xgboost_model],\n",
    "    sagemaker_session=sagemaker_session)\n",
    "\n",
    "endpoint_name = 'end-to-end-ml-sm-pipeline-endpoint-{0}'.format(str(int(time.time())))\n",
    "print(endpoint_name)\n",
    "\n",
    "pipeline_model.deploy(initial_instance_count=1, \n",
    "                      instance_type='ml.m5.2xlarge', \n",
    "                      endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Execute inference</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    serializer=CSVSerializer(),\n",
    "    deserializer=JSONDeserializer())\n",
    "\n",
    "payload = \"TID008,HAWT,64,80,46,21,55,55,7,34,SE\"\n",
    "print(predictor.predict(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can cleanup resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
