{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model Training</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use the Amazon SageMaker open source XGBoost container (https://github.com/aws/sagemaker-xgboost-container) to train a simple binary classification model, using the pre-processed data generated in the previous step by the processing job.\n",
    "Using XGBoost as a framework provides more flexibility than using it as a built-in algorithm as it enables more advanced scenarios that allow pre-processing and post-processing scripts or any kind of custom logic to be incorporated into your training script.\n",
    "\n",
    "Let's define the variables first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check SageMaker Python SDK version\n",
    "import sagemaker\n",
    "print(sagemaker.__version__)\n",
    "\n",
    "def versiontuple(v):\n",
    "    return tuple(map(int, (v.split(\".\"))))\n",
    "\n",
    "if versiontuple(sagemaker.__version__) < versiontuple('2.22.0'):\n",
    "    raise Exception(\"This notebook requires at least SageMaker Python SDK version 2.22.0. Please install it via pip.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "prefix = 'endtoendmlsm'\n",
    "\n",
    "print(region)\n",
    "print(role)\n",
    "print(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r experiment_name\n",
    "%store -r trial_name\n",
    "\n",
    "print(experiment_name)\n",
    "print(trial_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training code is implemented in the `source_dir/training.py` file.\n",
    "\n",
    "The script parses arguments that are passed when the XGBoost Docker container code invokes the script for execution. These arguments represent the hyperparameters that you specify when strarting the training job plus the location of training and validation data. Then, we load training and validation data and execute XGBoost training with the provided parameters.\n",
    "\n",
    "<strong>Note</strong>: this behavior, named Script Mode execution, is enabled by a library that is installed in the XGBoost container (sagemaker-training-toolkit, https://github.com/aws/sagemaker-training-toolkit) and facilitates the development of SageMaker-compatible Docker containers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Debugging\n",
    "## LossNotDecreasing\n",
    "\n",
    "Once we are confident our training script is working as expected (first execute the notebook without debugging enabled) and there are no major errors preventing its execution, we can enable debugging.\n",
    "\n",
    "During training, we will save the state of the tensors using Amazon SageMaker debugging features, and then analyze debugging outputs with jobs that are run while the training job is executed. For XGBoost, Amazon SageMaker debugging supports saving evaluation metrics, lebels and predictions, feature importances, and SHAP values.\n",
    "\n",
    "First, we need to modify our training script to enable Amazon SageMaker debugging. Note that this is required for the XGBoost framework, whilst for MXNet and Tensorflow debugging works also with no code changes.\n",
    "\n",
    "We created a Hook object which we pass as a callback function when creating a Booster. The Hook object is created by loading a JSON configuration that is available in a specific path in the Docker container (opt/ml/input/config/debughookconfig.json); this file is generated by Amazon SageMaker from the CreateTrainingJob() API call configuration. Note that Amazon SageMaker debugging is highly configurable, you can choose exactly what to save.\n",
    "\n",
    "Let's look at the modified script:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pygmentize source_dir/training_debug.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The modified script allows to capture tensors and save to Amazon S3, but doing this will not cause any debug analysis to run. In order to analyze debug outputs we need to configure the XGBoost estimator to define a collection of rules that will be run while the training job is executed.\n",
    "\n",
    "We are enabling a built-in (1P) debug rule named LossNotDecreasing which checks if the loss is not decreasing across step. In this scenario, we have chosen to run this rule at every step on the validation-rmse metric values: this means that the new rmse values must always go down at each step.\n",
    "\n",
    "When the estimator fit() method is called, Amazon SageMaker will start two jobs: a Training Job, where we also capture and save tensors, and a debug Processing Job (powered by Amazon SageMaker Processing Jobs), which will run in parallel and analyze tensor data to check if the rule conditions are met.\n",
    "\n",
    "Note that we are passing the Wait=False parameter to the fit() method to avoid waiting for the training job to complete and just fire and forget the API call.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our script ready, we can leverage on the XGBoost estimator of the Amazon SageMaker Python SDK to start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.xgboost import XGBoost\n",
    "from sagemaker.debugger import Rule, rule_configs, DebuggerHookConfig, CollectionConfig\n",
    "\n",
    "hyperparameters = {\n",
    "    \"max_depth\": \"3\",\n",
    "    \"eta\": \"0.1\",\n",
    "    \"gamma\": \"6\",\n",
    "    \"min_child_weight\": \"6\",\n",
    "    \"silent\": \"0\",\n",
    "    \"objective\": \"reg:logistic\",\n",
    "    \"num_round\": \"20\"\n",
    "}\n",
    "\n",
    "entry_point='training_debug.py'\n",
    "source_dir='source_dir/'\n",
    "output_path = 's3://{0}/{1}/output/'.format(bucket_name, prefix)\n",
    "debugger_output_path = 's3://{0}/{1}/output/debug'.format(bucket_name, prefix) # Path where we save debug outputs\n",
    "code_location = 's3://{0}/{1}/code'.format(bucket_name, prefix)\n",
    "\n",
    "hook_config = DebuggerHookConfig(\n",
    "    s3_output_path=debugger_output_path,\n",
    "    hook_parameters={\n",
    "        \"save_interval\": \"1\"\n",
    "    },\n",
    "    collection_configs=[\n",
    "        CollectionConfig(\"hyperparameters\"),\n",
    "        CollectionConfig(\"metrics\"),\n",
    "        CollectionConfig(\"predictions\"),\n",
    "        CollectionConfig(\"labels\"),\n",
    "        CollectionConfig(\"feature_importance\"),\n",
    "        CollectionConfig(\"average_shap\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "estimator = XGBoost(\n",
    "    base_job_name=\"end-to-end-ml-sm-xgb\",\n",
    "    entry_point=entry_point,\n",
    "    source_dir=source_dir,\n",
    "    output_path=output_path,\n",
    "    code_location=code_location,\n",
    "    hyperparameters=hyperparameters,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    framework_version=\"0.90-2\",\n",
    "    py_version=\"py3\",\n",
    "    role=role,\n",
    "    # Initialize your hook.\n",
    "    debugger_hook_config=hook_config,\n",
    "    \n",
    "    # Initialize your rules. These will read data for analyses from the path specified\n",
    "    # for the hook\n",
    "    rules=[Rule.sagemaker(rule_configs.loss_not_decreasing(),\n",
    "                         rule_parameters={\n",
    "                             # Rule does not use the default losses collection,\n",
    "                             # but uses a regex to look for specific tensor values\n",
    "                             \"use_losses_collection\": \"False\",\n",
    "                             \"tensor_regex\": \"validation-rmse\",\n",
    "                             \"collection_names\": \"metrics\",\n",
    "                             # Num steps is used to specify when to evaluate this rule (every num_steps)\n",
    "                             \"num_steps\" : \"1\"}\n",
    "                         )]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment tracking configuration\n",
    "experiment_config={\n",
    "    \"ExperimentName\": experiment_name,\n",
    "    \"TrialName\": trial_name,\n",
    "    \"TrialComponentDisplayName\": \"xgboost-training\",\n",
    "}\n",
    "\n",
    "train_config = sagemaker.TrainingInput('s3://{0}/{1}/data/preprocessed/train/'.format(\n",
    "    bucket_name, prefix), content_type='text/csv')\n",
    "val_config = sagemaker.TrainingInput('s3://{0}/{1}/data/preprocessed/val/'.format(\n",
    "    bucket_name, prefix), content_type='text/csv')\n",
    "\n",
    "estimator.fit({'train': train_config, 'validation': val_config},\n",
    "              experiment_config=experiment_config, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training job has started, we can check its debug configuration and status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "client = estimator.sagemaker_session.sagemaker_client\n",
    "\n",
    "description = client.describe_training_job(TrainingJobName=estimator.latest_training_job.name)\n",
    "print('Debug Hook configuration: ')\n",
    "print(description['DebugHookConfig'])\n",
    "print()\n",
    "print('Debug rules configuration: ')\n",
    "print(description['DebugRuleConfigurations'])\n",
    "print()\n",
    "print('Training job status')\n",
    "print(description['TrainingJobStatus'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get all the logs for the training job being executed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.logs_for_job(estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the same time, we can check the status of the rule execution job as follows. Note that this requires some time, so you might be interested in looking at the SageMaker Debugger documentation while this runs: https://github.com/awslabs/sagemaker-debugger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "client = estimator.sagemaker_session.sagemaker_client\n",
    "\n",
    "iterate = True\n",
    "while(iterate):\n",
    "    description = client.describe_training_job(TrainingJobName=estimator.latest_training_job.name)\n",
    "    eval_status = description['DebugRuleEvaluationStatuses'][0]\n",
    "    print(eval_status)\n",
    "    if eval_status['RuleEvaluationStatus'] != 'InProgress':\n",
    "        iterate = False\n",
    "    else:\n",
    "        time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review the configuration and logs of the rule execution job, executed by Amazon SageMaker Processing Jobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processing_job_arn = eval_status['RuleEvaluationJobArn']\n",
    "processing_job_name = processing_job_arn[processing_job_arn.rfind('/') + 1 :]\n",
    "print(processing_job_name)\n",
    "\n",
    "client = estimator.sagemaker_session.sagemaker_client\n",
    "descr = client.describe_processing_job(ProcessingJobName=processing_job_name)\n",
    "descr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sagemaker_session.logs_for_processing_job(descr['ProcessingJobName'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Debug Outputs\n",
    "\n",
    "In this section we will see how you can use the SDK to manually analyze debug outputs.\n",
    "\n",
    "First thing is creating a trial, which is the construct that allows accessing to tensors for a single training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install smdebug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smdebug.trials import create_trial\n",
    "\n",
    "s3_output_path = description[\"DebugHookConfig\"][\"S3OutputPath\"] + '/' + estimator.latest_training_job.name + '/debug-output/'\n",
    "print(s3_output_path)\n",
    "trial = create_trial(s3_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can print the list of all the tensors that were saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trial.tensor_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a specific tensor, we can ask at which steps we have data for the tensor. In this case, we have data for all steps since the frequency was set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial.tensor(\"validation-rmse\").steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial.tensor(\"train-rmse\").value(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial.tensor(\"predictions\").value(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance metrics\n",
    "\n",
    "We can also create a simple function that visualizes the training and validation errors as the training progresses. We expect each training errors to get smaller over time, as the system converges to a good solution. Now, remember that this is an interactive analysis - we are showing these tensors to give an idea of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "\n",
    "def get_data(trial, tname):\n",
    "    \"\"\"\n",
    "    For the given tensor name, walks though all the iterations\n",
    "    for which you have data and fetches the values.\n",
    "    Returns the set of steps and the values.\n",
    "    \"\"\"\n",
    "    tensor = trial.tensor(tname)\n",
    "    steps = tensor.steps()\n",
    "    vals = [tensor.value(s) for s in steps]\n",
    "    return steps, vals\n",
    "\n",
    "def plot_collection(trial, collection_name, regex='.*', figsize=(8, 6)):\n",
    "    \"\"\"\n",
    "    Takes a `trial` and a collection name, and \n",
    "    plots all tensors that match the given regex.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    sns.despine()\n",
    "    #print(collection_name)\n",
    "    tensors = trial.collection(collection_name).tensor_names\n",
    "    #print(tensors)\n",
    "    for tensor_name in sorted(tensors):\n",
    "        #print(tensor_name)\n",
    "        if re.match(regex, tensor_name):\n",
    "            steps, data = get_data(trial, tensor_name)\n",
    "            #print(np.mean(np.abs(data)))\n",
    "            #if np.mean(np.abs(data)) > 0.001 :\n",
    "            ax.plot(steps, data, label=tensor_name)\n",
    "            \n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    ax.set_xlabel('Iteration')\n",
    "    \n",
    "def plot_tensors(trial, tensors, regex='.*', figsize=(8, 6)):\n",
    "    \"\"\"\n",
    "    Takes a `trial` and tensor names, and \n",
    "    plots all tensors that match the given regex.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    sns.despine()\n",
    "    #print(tensors)\n",
    "    #tensors = trial.collection(collection_name).tensor_names\n",
    "    #print(tensors)\n",
    "    for tensor_name in sorted(tensors):\n",
    "        #print(tensor_name)\n",
    "        if re.match(regex, tensor_name):\n",
    "            steps, data = get_data(trial, tensor_name)\n",
    "            #if np.mean(np.abs(data)) > 0.05 :\n",
    "            ax.plot(steps, data, label=tensor_name)\n",
    "\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    ax.set_xlabel('Iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_to_plot = [\"train-rmse\", \"validation-rmse\"]\n",
    "for metric in metrics_to_plot:\n",
    "    steps, data = get_data(trial, metric)\n",
    "    plt.plot(steps, data, label=metric)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Classification error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance\n",
    "\n",
    "We can also visualize the feature importances as determined by xgboost.get_fscore(). Note that feature importances with zero values are not included here (which means that those features were not used in any split conditions).\n",
    "\n",
    "For more information on the metrics related to feature importance in XGBoost, please visit: https://towardsdatascience.com/be-careful-when-interpreting-your-features-importance-in-xgboost-6e16132588e7\n",
    "\n",
    "weight is the number of times a feature is used to split the data across all trees\n",
    "gain represents fractional contribution of each feature to the model based on the total gain of this feature's splits. Higher percentage means a more important predictive feature\n",
    "cover is a metric of the number of observation related to this feature\n",
    "total_gain is the total gain across all splits the feature is used in\n",
    "total_cover is the total coverage across all splits the feature is used in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "    \n",
    "def plot_feature_importance(trial, collection_name, step, metric):\n",
    "    feature_importance_tensors = trial.collection(collection_name).tensor_names\n",
    "\n",
    "    feature_names = []\n",
    "    feature_values = []\n",
    "    \n",
    "    plt.subplots(figsize=(18,7))\n",
    "    \n",
    "    for tensor_name in feature_importance_tensors:\n",
    "        if tensor_name.find('/' + metric) >= 0:\n",
    "            index = tensor_name.rfind('/')\n",
    "            feature_name = tensor_name[index+1:]\n",
    "            feature_names.append(feature_name)\n",
    "            tensor = trial.tensor(tensor_name)\n",
    "            value_at_step = tensor.value(step)[0]\n",
    "            feature_values.append(value_at_step)\n",
    "\n",
    "    pos = range(len(feature_values))\n",
    "    plt.bar(pos, feature_values, color='g')\n",
    "    plt.xlabel('Features', fontsize=16)\n",
    "    plt.ylabel('Feature Importance ({0})'.format(metric), fontsize=16)\n",
    "    plt.xticks(pos, feature_names)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(trial, \"feature_importance\", 19, \"gain\")\n",
    "plot_feature_importance(trial, \"feature_importance\", 19, \"cover\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_collection(trial, \"average_shap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment analytics\n",
    "\n",
    "You can visualize experiment analytics either from Amazon SageMaker Studio Experiments plug-in or using the SDK from a notebook, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "\n",
    "analytics = ExperimentAnalytics(experiment_name=experiment_name)\n",
    "analytics.dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training is completed, the serialized model will be saved in the S3 `output_location` defined above.\n",
    "You can now move to the next notebook in the **04_deploy_model** folder to see how to use that model for inference."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
